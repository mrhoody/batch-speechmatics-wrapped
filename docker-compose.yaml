networks:
  sm_batch_transcriber:
    driver: bridge

services:
  sm_batch_wrapped:
    build:
      context: .
      dockerfile: ./batch_CPU.Dockerfile
    container_name: sm_batch_wrapped
    ports:
      - 8000:8000
    networks:
      - sm_batch_transcriber
    depends_on:
      - sm_gpu
    environment:
      - SM_INFERENCE_SERVER_URL=sm_gpu:8001
      - LICENSE_TOKEN=${LICENSE_TOKEN}
  sm_gpu:
    image: speechmatics-docker-demo.jfrog.io/sm-gpu-inference-server-en-singapore_poc
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities:
              - gpu
    container_name: sm_gpu
    networks:
      - sm_batch_transcriber
    ports:
      - 8001:8001
    environment:
      - LICENSE_TOKEN=${LICENSE_TOKEN}
      # - SM_MAX_CONCURRENT_CONNECTIONS=10
      - NVIDIA_DRIVER_CAPABILITIES=all
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
